
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import random
import argparse
import sys




#sys.path.append('/Users/Charles/Documents/Github/tensorflow/')
#print(sys.path)
from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf 
import matplotlib.pyplot as plt
import numpy as np

FLAGS = None
def plotMatrix(matrix,r,c):


	
	fig = plt.figure(1)
	for plotIndex in range(10):
		ax = fig.add_subplot(4, 3, plotIndex +1 ) # needs to start at 1 not 0
		plotMat = matrix[plotIndex,:]
		#(r,c) = matrix.shape
		x = []
		y = []
		color = []
		for i in range(r):
			for k in range(c):
				index = i * r + k 
				x.append(k)
				y.append(r - i)
				color.append(plotMat[index])
		rgb = plt.get_cmap('jet')(color)

		ax.scatter(x, y, color = rgb)
	plt.show()

def findAccuracy(m,prediction,actual):
	predictionNum = np.zeros((m,1))
	actualNum = np.zeros((m,1))
	for i in range(len(prediction)):
		x = prediction[i]
		idx = np.where(x==x.max())[1]
		if len(idx) > 1:
			idx = idx[0]
		predictionNum[i] = idx
		#print(idx)
		y = actual[i]
		idx = np.where(y==y.max())[1]
		actualNum[i] = idx

	visualComparison = np.hstack((actualNum,predictionNum))
	print(visualComparison[0:5])
	error =  actualNum == predictionNum
	accuracy = np.mean(error)
	print(accuracy)

def predict(x,theta1,theta2):
	a2 = activationFunction(x,theta1)
	return activationFunction(a2,theta2)

def activationFunction(x,theta):
	z  = np.dot(x, theta)
	g = 1 / (1 + np.exp(-z)) #sigmoid function
	#g_deriv = np.multiply(g ,(1-g))
	return( g )#, g_deriv)  

def costFunction(g,y,m):
	J = - (1 / m) *  np.sum(np.multiply(y , np.log(g))  +  np.multiply((1 - y) , np.log(1 - g)))
	#J +=  ((_lambda / (2 * m)) * np.sum(np.square(theta)) )
	return(J)

def train(theta1,theta2,x,y,m,a):
	(r1,c1) = theta1.shape
	(r2,c2) = theta2.shape
	delta1 = np.zeros((r1,c1))
	delta2 = np.zeros((r2,c2))

	#print(y)
	for i in range(m):
		y3 = y[i]
		a1 = x[i]
		a2 = activationFunction(a1,theta1)
		a3 =  activationFunction(a2,theta2)

		sigma3 = a3 - y3 # error 
		#sigma3 = np.dot( sigma4, np.transpose(theta2))#* a3 * (1 - a3)
		sigma2 = np.multiply(np.dot( sigma3, np.transpose(theta2))  ,  np.multiply(a2, (1- a2)))

		delta2 += np.dot( np.transpose(a2) , sigma3 ) 
		delta1 += np.dot( np.transpose(a1) , sigma2 )
		
	# preventing the bias unit from being regularized
	#thetatemp = theta 
	#thetatemp[0,:] = 0
	delta1 =   a * (1 / m) * ( delta1 )
	theta1 = theta1 - delta1 

	delta2 =   a * (1 / m) * ( delta2 )
	theta2 = theta2 - delta2

	return( theta1, theta2 )

#function declartion for main function
def main(_):

	print('begin')



	mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

	#for i in range(100):
	m = 1# of training data per training step
	a =  0.01  # learning rate
	_lambda = 0.01
	training_steps = 55000
	num_hidden_inputs = 40


	batchx , batchy =  mnist.train.next_batch( m ) # x is 784 for  a 28x28 image y is
	matx = np.matrix(batchx)

	maty = np.matrix(batchy)
	one = np.ones((m,1))
	matx = np.hstack((one,matx))
	num_features = matx.shape[1]
	num_output = maty.shape[1]
	print("Number of features: %d Number of Output: %d"%(num_features,num_output))
	#Creating the theta vector
	theta1 = np.random.rand(num_features, num_hidden_inputs) / 100
	theta2 = np.random.rand(num_hidden_inputs, num_output) /100
	print(theta2[0:10,0:10])
	# print(theta.shape)
	# print(matx.shape)
	print("test cost")
	g = predict(matx,theta1,theta2)

	J = costFunction(g,maty,m)

	print(J)

	for i in range(training_steps):
		print("Step %d"%(i))


		batchx , batchy =  mnist.train.next_batch( m ) # x is 784 for  a 28x28 image y is
		matx = np.matrix(batchx) 
		maty = np.matrix(batchy)
		one = np.ones((m,1))
		matx = np.hstack((one,matx))

		g = predict(matx,theta1,theta2)
		findAccuracy(m,g,maty) # prints out the accuracy and a visual comparison

		theta1, theta2 = train(theta1,theta2,matx,maty,m,a)
		J = costFunction(g,maty,m)
		#print(J) #check if algorithm is working




	#accuracy
	num_test = 1000
	batchx , batchy =  mnist.test.next_batch( num_test ) # x is 784 for  a 28x28 image y is
	matx = np.matrix(batchx) 
	maty = np.matrix(batchy)
	one = np.ones((num_test,1))
	matx = np.hstack((one,matx))
	g = predict(matx,theta1,theta2)
	findAccuracy(num_test,g,maty)
	#Put thetas into a csv file for inspection
	a = np.asarray(theta1)
	np.savetxt("theta1.csv", a, delimiter=",")
	a = np.asarray(theta2)
	np.savetxt("theta2.csv", a, delimiter=",")

	theta2 = theta2 
	print("Bias unit %d")
	print(theta2[0,:])
	print("Other units")
	#print(theta[:,0])
	theta = np.transpose(theta2  )



	#plotMatrix(theta1,784,784,6,plot1)
	plotMatrix(theta,5,8)



	#mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
	#  55,000 training data mnist.train 10,000 test data mnist.test 5,000 validation data mnist.validation 
	# mnist.train.labels has the labels 0-9 of the training data


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',
                      help='Directory for storing input data')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)